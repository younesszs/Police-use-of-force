{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d799db-c967-41ed-b3cf-40c748321853",
   "metadata": {},
   "source": [
    "# Chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeb7fee-18ef-470c-b6a3-c9e2252f92e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from docplex.mp.model import Model\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "from scipy.stats import pearsonr\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a8f6c1-709b-434a-af6e-7c1cc68b28f5",
   "metadata": {},
   "source": [
    "### Data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0f0b88-5fc9-4af4-b49d-1d852af2d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_complaints_complaints = pd.read_csv('complaints-complaints.csv')\n",
    "data_investigator = pd.read_csv('investigators.csv')\n",
    "\n",
    "merged_data = pd.merge(data_investigator, data_complaints_complaints, on=['cr_id'])\n",
    "merged_data['full name'] = merged_data['first_name'] + ' ' + merged_data['last_name']\n",
    "merged_data['combined time'] = merged_data['incident_date'].astype(str) + ' ' + merged_data['incident_time']\n",
    "features = ['cr_id', 'beat', 'full name', 'combined time']\n",
    "df = merged_data[features]\n",
    "df = df.dropna()\n",
    "df['combined time'] = pd.to_datetime(df['combined time'], errors='coerce')\n",
    "df.sort_values(by = 'combined time')\n",
    "df = df[(df['combined time'] >= '2022-01-01') & (df['combined time'] <= '2022-12-31')]\n",
    "\n",
    "print(len(df['full name'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f089ad2d-d3d4-4998-bd7b-cfc089d9f3e1",
   "metadata": {},
   "source": [
    "### Construct officers' actual pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775a3fc-ad9a-403f-bf78-286936f611b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# each officer should have M+1 total connections (including self‐loop)\n",
    "M = 8\n",
    "\n",
    "# 3) Index officers\n",
    "officers = sorted(df['full name'].unique())\n",
    "idx = {name: i for i, name in enumerate(officers)}\n",
    "n = len(officers)\n",
    "\n",
    "potential = np.zeros((n, n), dtype=int)\n",
    "np.fill_diagonal(potential, 1)\n",
    "for _, group in df.groupby(['cr_id', 'beat', 'combined time']):\n",
    "    names = group['full name'].unique()\n",
    "    ids = [idx[name] for name in names]\n",
    "    for i in ids:\n",
    "        for j in ids:\n",
    "            potential[i, j] = 1\n",
    "\n",
    "mdl = Model(name='officer_adj')\n",
    "\n",
    "alpha = mdl.binary_var_matrix(n, n, name='α')\n",
    "\n",
    "for i in range(n):\n",
    "    mdl.add_constraint(alpha[i, i] == 1, ctname=f\"diag_{i}\")\n",
    "\n",
    "for i in range(n):\n",
    "    mdl.add_constraint(mdl.sum(alpha[i, j] for j in range(n)) == M+1,\n",
    "                       ctname=f\"rowdeg_{i}\")\n",
    "for j in range(n):\n",
    "    mdl.add_constraint(mdl.sum(alpha[i, j] for i in range(n)) == M+1,\n",
    "                       ctname=f\"coldeg_{j}\")\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        mdl.add_constraint(alpha[i, j] == alpha[j, i],\n",
    "                           ctname=f\"sym_{i}_{j}\")\n",
    "        \n",
    "sol = mdl.solve(log_output=False)\n",
    "if sol is None:\n",
    "    raise RuntimeError(f\"No feasible adjacency for M={M}\")\n",
    "\n",
    "alpha_before = np.zeros((n, n), dtype=int)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        alpha_before[i, j] = int(alpha[i, j].solution_value)\n",
    "\n",
    "print(\"Officer index → name:\")\n",
    "for i, name in enumerate(officers):\n",
    "    print(f\"{i}: {name}\")\n",
    "print(f\"\\nAdjacency matrix shape: {alpha_before.shape}\")\n",
    "print(alpha_before)\n",
    "print(\"\\nRow sums:\", alpha_before.sum(axis=1))\n",
    "print(\"Col sums:\", alpha_before.sum(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6644a23d-33b7-4405-a1af-33321693bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the symmetry of the matrix\n",
    "is_symmetric = np.array_equal(alpha_before, alpha_before.T)\n",
    "print(\"Is alpha symmetric?  \", is_symmetric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c3f968-c341-4179-b177-6e92506673dd",
   "metadata": {},
   "source": [
    "### Construct timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdbd15a-073b-4b38-9314-8f67b2475395",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['combined times'] = pd.to_datetime(df['combined time'])\n",
    "\n",
    "earliest_timestamp = min(df['combined time'])\n",
    "df['relative times in days'] = (df['combined time'] - earliest_timestamp).dt.total_seconds() / (24*60*60)\n",
    "grouped = df.groupby('full name')['relative times in days'].apply(list)\n",
    "timestamps_per_officer = grouped.to_dict()\n",
    "timestamps = list(timestamps_per_officer.values())\n",
    "for i in range(len(timestamps)):\n",
    "    timestamps[i] = np.sort(timestamps[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca3a82-72f8-47f9-8801-32312458b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_estimation(alpha, events, epochs_num):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Flatten into two 1D tensors: times (float) and marks (long)\n",
    "    all_times = []\n",
    "    all_marks = []\n",
    "    for k, ev in enumerate(events):\n",
    "        all_times.append(torch.tensor(ev, dtype=torch.double))\n",
    "        all_marks.append(torch.full((len(ev),), k, dtype=torch.long))\n",
    "    times_unsorted = torch.cat(all_times)        # shape (N,)\n",
    "    marks_unsorted = torch.cat(all_marks)        # shape (N,)\n",
    "    \n",
    "    # sort by time\n",
    "    times, order = torch.sort(times_unsorted)\n",
    "    marks = marks_unsorted[order]\n",
    "    \n",
    "    # observation window\n",
    "    T = int(max([max(timestamps[i]) for i in range(len(events))]))\n",
    "    \n",
    "    # known branching matrix α (as a NumPy array or list of lists)\n",
    "    alpha_np = np.array(alpha)\n",
    "    D = alpha_np.shape[0]\n",
    "\n",
    "    \n",
    "    # convert α to a torch tensor once\n",
    "    alpha = torch.as_tensor(alpha_np, dtype=torch.double)\n",
    "    \n",
    "    # --- 2) Initialize parameters to estimate -------------------------------\n",
    "    init_mu    = np.round(np.random.rand(D), 3)   \n",
    "    init_theta = np.round(np.random.rand(D), 3)   \n",
    "    init_omega = np.round(np.random.rand(D, D), 3) \n",
    "    \n",
    "    # we work in the unconstrained space and exponentiate inside the loop\n",
    "    mu_param    = torch.log(torch.tensor(init_mu,    dtype=torch.double)).requires_grad_()\n",
    "    theta_param = torch.log(torch.tensor(init_theta,    dtype=torch.double)).requires_grad_()\n",
    "    omega_param = torch.log(torch.tensor(init_omega,    dtype=torch.double)).requires_grad_()\n",
    "    \n",
    "    optimizer = optim.Adam([mu_param, theta_param, omega_param], lr=1e-6)\n",
    "    \n",
    "    # --- 3) Training loop ------------------------------------------------------\n",
    "    \n",
    "    dt = 0.1\n",
    "    for epoch in range(epochs_num):\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # enforce positivity\n",
    "        mu_pos    = torch.exp(mu_param)           # shape (D,)\n",
    "        theta_pos = torch.exp(theta_param)        # shape (D,)\n",
    "        omega_pos = torch.exp(omega_param)    # shape (D,D)\n",
    "    \n",
    "        N = times.numel()\n",
    "    \n",
    "        # point‐process term: sum_i log λ_{mark_i}(time_i)\n",
    "        dt_mat = times.unsqueeze(1) - times.unsqueeze(0)    # (N,N)\n",
    "        causal = (dt_mat > 0).double()                      # mask past events\n",
    "    \n",
    "        # get indices for α, ω, θ\n",
    "        mi = marks                                       # (N,)\n",
    "        mj = marks.unsqueeze(0).expand(N, N)             # (N,N)\n",
    "        mi_mat = mi.unsqueeze(1).expand(N, N)            # (N,N)\n",
    "    \n",
    "        alpha_ij = alpha[mi_mat, mj]                         # (N,N)\n",
    "        omega_ij = omega_pos[mi_mat, mj]                     # (N,N)\n",
    "        theta_i  = theta_pos[mi].unsqueeze(1)                # (N,1)\n",
    "    \n",
    "        kern = alpha_ij * theta_i * omega_ij * torch.exp(-omega_ij * dt_mat) * causal\n",
    "        sum_kern = kern.sum(dim=1)                       # (N,)\n",
    "    \n",
    "        lam_i = mu_pos[mi] + sum_kern                      # (N,)\n",
    "        logL1 = torch.log(lam_i).sum()\n",
    "    \n",
    "        # 3b) compensator term\n",
    "        dtT = (T - times).unsqueeze(0)                   # (1,N)\n",
    "        Aθ = alpha * theta_pos.unsqueeze(1)              # (D,D)\n",
    "    \n",
    "        Aθ_m = Aθ[:, marks]                              # (D,N)\n",
    "        omega_m  = omega_pos[:, marks]                       # (D,N)\n",
    "    \n",
    "        term = Aθ_m * (1 - torch.exp(-omega_m * dtT))        # (D,N)\n",
    "        sum_term = term.sum(dim=1)                       # (D,)\n",
    "    \n",
    "        intL = mu_pos * T + sum_term                     # (D,)\n",
    "        logL2 = intL.sum()\n",
    "    \n",
    "        # negative log-likelihood\n",
    "        nll = logL2 - logL1\n",
    "        nll.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        #print(f'epochs = {epoch} NLL = {nll.item():.4f}')\n",
    "        if epoch % 200 == 0:\n",
    "            print(f\"Epoch {epoch:4d}  NLL = {nll.item():.4f}\")\n",
    "    \n",
    "    # --- 4) Extract estimates -------------------------------------------------\n",
    "    \n",
    "    mu_est    = torch.exp(mu_param).detach().numpy()\n",
    "    theta_est = torch.exp(theta_param).detach().numpy()\n",
    "    omega_est = torch.exp(omega_param).detach().numpy()\n",
    "    \n",
    "    return mu_est, theta_est, omega_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3138bf77-d227-4da6-a533-305fb359c00a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mu_est, theta_est, omega_est = parameter_estimation(alpha_before, timestamps, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca1596-3903-417b-8490-52b83e9eb942",
   "metadata": {},
   "source": [
    "### Compute stationary intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936b0037-3de2-4abe-9355-595b4f5a4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = int(max([max(timestamps[i]) for i in range(len(timestamps))]))\n",
    "def intensity(events,  T, alpha, theta, omega, mu, dt = 0.1):\n",
    "    nodes = len(events)\n",
    "    t = np.arange(0, T, dt)\n",
    "    alpha = np.asarray(alpha)\n",
    "    omega = np.asarray(omega)\n",
    "    theta = np.asarray(theta)\n",
    "    mu = np.asarray(mu)\n",
    "    #evs = [np.asarray(ev) for ev in events]\n",
    "    lam = np.zeros((nodes, len(t)))\n",
    "\n",
    "    for n in range(nodes):\n",
    "        for i in range(len(t)):\n",
    "            lam[n, i] = mu[n]\n",
    "            for j in range(nodes):\n",
    "                for k in range(len(events[j])):\n",
    "                    if t[i] > events[j][k]:\n",
    "                        lam[n, i] += alpha[n, j] * theta[j] * omega[n, j] * np.exp(-omega[n, j]*(t[i] - events[j][k]))\n",
    "    return lam\n",
    "intensity = intensity(timestamps,  T, alpha_before, theta_est, omega_est, mu_est, dt = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a7220-29a8-4832-847d-05e3a47076c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_stationary = [np.mean(intensity[i]) for i in range(len(intensity))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bad7e1-3c1f-4a70-990f-9d36b7cc0d6b",
   "metadata": {},
   "source": [
    "### Optimize the network using CPLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb335ef-e853-47a4-b079-18b324721843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cp(M, mu, theta):\n",
    "\n",
    "    n = len(mu)\n",
    "    # --- MODEL ---\n",
    "    mdl = Model(name='hawkes_adj_opt')\n",
    "    mdl.set_log_output(True)\n",
    "    mdl.parameters.timelimit.set(8000)     # set timelimit\n",
    "\n",
    "    \n",
    "    # Decision vars\n",
    "    alpha = {(i,j): mdl.binary_var(name=f\"alpha_{i}_{j}\")\n",
    "             for i in range(n) for j in range(n)}    \n",
    "    g     = {(i,j): mdl.continuous_var(lb=0, name=f\"g_{i}_{j}\")\n",
    "             for i in range(n) for j in range(n)}\n",
    "    \n",
    "    # Diagonal ones\n",
    "    for i in range(n):\n",
    "        mdl.add_constraint(alpha[i,i] == 1, ctname=f\"diag_{i}\")\n",
    "    \n",
    "    # Degree constraints\n",
    "    for i in range(n):\n",
    "        mdl.add_constraint(mdl.sum(alpha[i,j] for j in range(n)) == M+1,\n",
    "                           ctname=f\"outdeg_{i}\")\n",
    "    for j in range(n):\n",
    "        mdl.add_constraint(mdl.sum(alpha[i,j] for i in range(n)) == M+1,\n",
    "                           ctname=f\"indeg_{j}\")\n",
    "    \n",
    "    # Indicator constraints for g_{ij} = alpha_{ij} * theta_j * λ̄_j\n",
    "    #    where λ̄_j = mu[j] + sum_k g[j,k]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            # compute λ̄_j as a linear expr in the g-variables\n",
    "            lam_bar_j = mu[j] + mdl.sum(g[j,k] for k in range(n))\n",
    "            # if alpha=1, enforce g = theta_j * lambda_bar_j\n",
    "            mdl.add_indicator(alpha[i,j], g[i,j] == theta[j] * lam_bar_j,\n",
    "                              name=f\"ind_on_{i}_{j}\")\n",
    "            # if alpha=0, enforce g=0\n",
    "            mdl.add_indicator(alpha[i,j], g[i,j] == 0,\n",
    "                             active_value=0, name=f\"ind_off_{i}_{j}\")\n",
    "\n",
    "    # Symmetry: α[i,j] == α[j,i]\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            mdl.add_constraint(alpha[i,j] == alpha[j,i],\n",
    "                               ctname=f\"sym_{i}_{j}\")\n",
    "\n",
    "    \n",
    "    # Objective: minimize sum of all g_{ij}\n",
    "    mdl.minimize(mu[i] + mdl.sum(g[i,j] for i in range(n) for j in range(n)))\n",
    "    \n",
    "    \n",
    "    # Solve\n",
    "    sol = mdl.solve()\n",
    "    if sol is None:\n",
    "        raise RuntimeError(\"No feasible solution found\")\n",
    "\n",
    "    g_sol     = np.array([[sol.get_value(g[i,j])\n",
    "                       for j in range(n)]\n",
    "                      for i in range(n)])\n",
    "    \n",
    "    # Extract optimized alpha matrix\n",
    "    alpha_opt = [[int(alpha[i,j].solution_value) for j in range(n)]\n",
    "                 for i in range(n)]\n",
    "    alpha_opt = np.array(alpha_opt)\n",
    "    return alpha_opt, g_sol\n",
    "#rescale theta to preserve stationarity of the model\n",
    "slack = 1e-6\n",
    "target = (1 - slack) / M\n",
    "scale  = target / max(theta_est)\n",
    "theta_scaled = [t * scale for t in theta_est]\n",
    "alpha_after, _ = cp(M, mu_est, theta_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feade790-1fcb-453e-9c7b-4d48bf5eb3b5",
   "metadata": {},
   "source": [
    "### The different baseline methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46226d43-8014-4abc-b8f9-834ec34d27a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Hawkes\n",
    "def generate_hp_before(nodes, T, alpha, theta, omega, mu):\n",
    "\n",
    "    mu = np.asarray(mu)\n",
    "    alpha = np.asarray(alpha)\n",
    "    omega = np.asarray(omega)\n",
    "    theta = np.asarray(theta)\n",
    "\n",
    "    #1. Generate immigrants\n",
    "    N0 = np.random.poisson(mu.sum()*T)\n",
    "    # Uniform times on [0, T] \n",
    "    imm_times = np.random.rand(N0)*T\n",
    "    #Marks drawn with probability p(k) = mu / sum(mu)\n",
    "    imm_marks = np.random.choice(nodes, size = N0, p = mu/mu.sum())\n",
    "    #events = list of (times, marks)\n",
    "    events = list(zip(imm_times, imm_marks))\n",
    "\n",
    "    #2. Recursivly generate offspring\n",
    "    cnt = 0\n",
    "    while cnt < N0:\n",
    "        t_parent, k_parent = events[cnt]\n",
    "        #for each possible children type j\n",
    "        for j in range(nodes):\n",
    "            # mean number of children in type j = alpha[j, k_parent]*theta[j]\n",
    "            m = np.random.poisson(alpha[j, k_parent]*theta[j])\n",
    "            if m > 0:\n",
    "                #draw m exponential delays with rate omega[j, k_parent]\n",
    "                delays = np.random.exponential(1.0/(omega[j, k_parent] + 1e-08), size = m)\n",
    "                for d in delays:\n",
    "                    t_child = t_parent + d\n",
    "                    if t_child < T:\n",
    "                        events.append((t_child, j))\n",
    "\n",
    "        cnt += 1\n",
    "\n",
    "    #3. Sort the events and split by type \n",
    "    events.sort(key = lambda x: x[0])\n",
    "    events_by_node = [[] for _ in range(nodes)]\n",
    "    for t, k in events:\n",
    "        events_by_node[k].append(t)\n",
    "\n",
    "    return events_by_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7f08a1-6eab-4570-9561-d55c8fa0fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hawkes MILP\n",
    "def generate_hp_after(nodes, T, alpha, theta, omega, mu, lam_bar):\n",
    "\n",
    "    mu = np.asarray(mu)\n",
    "    alpha = np.asarray(alpha)\n",
    "    omega = np.asarray(omega)\n",
    "    theta = np.asarray(theta)\n",
    "\n",
    "    #1. Generate immigrants\n",
    "    N0 = np.random.poisson(mu.sum()*T)\n",
    "    # Uniform times on [0, T] \n",
    "    imm_times = np.random.rand(N0)*T\n",
    "    #Marks drawn with probability p(k) = mu / sum(mu)\n",
    "    imm_marks = np.random.choice(nodes, size = N0, p = mu/mu.sum())\n",
    "    #events = list of (times, marks)\n",
    "    events = list(zip(imm_times, imm_marks))\n",
    "\n",
    "    #2. Recursivly generate offspring\n",
    "    cnt = 0\n",
    "    while cnt < N0:\n",
    "        t_parent, k_parent = events[cnt]\n",
    "        #for each possible children type j\n",
    "        for j in range(nodes):\n",
    "\n",
    "    #incorporate optimized lam\n",
    "            mean_offspring = alpha[j, k_parent] * theta[j] * lam_bar[j]\n",
    "            if mean_offspring > 0:\n",
    "                m = np.random.poisson(mean_offspring)\n",
    "                if m > 0:\n",
    "                    delays = np.random.exponential(1.0 / (omega[j, k_parent] + 1e-8), size=m)\n",
    "                    for d in delays:\n",
    "                        t_child = t_parent + d\n",
    "                        if t_child < T:\n",
    "                            events.append((t_child, j))\n",
    "        cnt += 1\n",
    "\n",
    "    #3. Sort the events and split by type \n",
    "    events.sort(key = lambda x: x[0])\n",
    "    events_by_node = [[] for _ in range(nodes)]\n",
    "    for t, k in events:\n",
    "        events_by_node[k].append(t)\n",
    "\n",
    "    return events_by_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257fa2a4-3f71-47d9-9eab-271db7a3e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Greedy Heuristics\n",
    "def greedy_influence_removal_iterative(alpha, events_by_node, theta, omega, M):\n",
    "    alpha = np.asarray(alpha)\n",
    "    omega = np.asarray(omega)\n",
    "    theta = np.asarray(theta)\n",
    "    \n",
    "    n = alpha.shape[0]\n",
    "    alpha_mod = alpha.copy()\n",
    "    \n",
    "    def compute_scores(alpha_mat):\n",
    "        # score[(i,j)] = total influence of edge (i,j)\n",
    "        scores = {}\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                if alpha_mat[i,j] == 1:\n",
    "                    s = 0.0\n",
    "                    # j → i contributions\n",
    "                    for t_j in events_by_node[j]:\n",
    "                        for t_i in events_by_node[i]:\n",
    "                            if t_j < t_i:\n",
    "                                dt = t_i - t_j\n",
    "                                s += theta[i] * omega[i,j] * np.exp(-omega[i,j] * dt)\n",
    "                    # i → j contributions\n",
    "                    for t_i in events_by_node[i]:\n",
    "                        for t_j in events_by_node[j]:\n",
    "                            if t_i < t_j:\n",
    "                                dt = t_j - t_i\n",
    "                                s += theta[j] * omega[j,i] * np.exp(-omega[j,i] * dt)\n",
    "                    scores[(i,j)] = s\n",
    "        return scores\n",
    "\n",
    "    # Iteratively remove the edge with highest score\n",
    "    for _ in range(M):\n",
    "        scores = compute_scores(alpha_mod)\n",
    "        if not scores:\n",
    "            break\n",
    "        # find edge with max score\n",
    "        (i_best, j_best), _ = max(scores.items(), key=lambda kv: kv[1])\n",
    "        # drop it\n",
    "        alpha_mod[i_best, j_best] = 0\n",
    "        alpha_mod[j_best, i_best] = 0\n",
    "    \n",
    "    return alpha_mod\n",
    "alpha_greedy = greedy_influence_removal_iterative(alpha_before, timestamps, theta_est, omega_est, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e0fc7-1685-40bd-b057-dcf05321a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random permutation\n",
    "def random_officer_reassignment(mu, theta, omega, alpha, events_by_node):\n",
    "    D = len(mu)\n",
    "    perm = np.random.permutation(D)\n",
    "\n",
    "    # Permute mu and theta\n",
    "    mu_perm = mu[perm]\n",
    "    theta_perm = theta[perm]\n",
    "\n",
    "    # Permute both axes of alpha and omega\n",
    "    alpha_perm = alpha[perm][:, perm]\n",
    "    omega_perm = omega[perm][:, perm]\n",
    "\n",
    "    # Permute event traces\n",
    "    events_by_node_perm = [[] for _ in range(D)]\n",
    "    for old_idx, times in enumerate(events_by_node):\n",
    "        new_idx = perm[old_idx]\n",
    "        events_by_node_perm[new_idx] = times.copy()  # preserve times, relabel node\n",
    "    \n",
    "    return mu_perm, theta_perm, omega_perm, alpha_perm, events_by_node_perm\n",
    "mu_r, theta_r, omega_r, alpha_r, events_r = random_officer_reassignment(mu_est, theta_est, omega_est, alpha_before, timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495c758a-aabc-4768-a0d3-cfe17e84c55b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Rotation Policy\n",
    "def simulate_rotation_policy(D, T, alpha, theta, omega, mu, delta_t):\n",
    "    \"\"\"\n",
    "    Simulate a Hawkes process where every delta_t, officers are reassigned.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    D : int - number of officers/nodes\n",
    "    T : float - total time\n",
    "    alpha, theta, omega, mu : MHP parameters\n",
    "    delta_t : float - time interval for reassignment\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    events_by_node : list of lists - event times per officer\n",
    "    \"\"\"\n",
    "    alpha = np.asarray(alpha)\n",
    "    theta = np.asarray(theta)\n",
    "    omega = np.asarray(omega)\n",
    "    mu = np.asarray(mu)\n",
    "    total_events = [[] for _ in range(D)]\n",
    "    time_offset = 0.0\n",
    "\n",
    "    while time_offset < T:\n",
    "        dt = min(delta_t, T - time_offset)\n",
    "\n",
    "        # permute officer labels\n",
    "        perm = np.random.permutation(D)\n",
    "        mu_p = mu[perm]\n",
    "        theta_p = theta[perm]\n",
    "        omega_p = omega[np.ix_(perm, perm)]\n",
    "        alpha_p = alpha[np.ix_(perm, perm)]\n",
    "\n",
    "        # simulate in this interval\n",
    "        ev = generate_hp_before(D, dt, alpha_p, theta_p, omega_p, mu_p)\n",
    "\n",
    "        # shift timestamps and assign back to fixed officer IDs\n",
    "        for i in range(D):\n",
    "            true_id = perm[i]\n",
    "            shifted_events = [time_offset + t for t in ev[i]]\n",
    "            total_events[true_id].extend(shifted_events)\n",
    "\n",
    "        time_offset += dt\n",
    "\n",
    "    # final sort\n",
    "    for i in range(D):\n",
    "        total_events[i].sort()\n",
    "    return total_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaebf89-efa5-4282-b17f-3e9bc22ed56c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nodes = len(df['full name'].unique())\n",
    "T = int(max([max(timestamps[i]) for i in range(len(timestamps))]))\n",
    "\n",
    "before_lenghts_array = []\n",
    "after_lenghts_array = []\n",
    "list_events_greedy = []\n",
    "list_events_random_perm = []\n",
    "rot = []\n",
    "\n",
    "for sim in range(1000):\n",
    "    events_before = generate_hp_before(nodes, T, alpha_before, theta_est, omega_est, mu_est)\n",
    "    events_after = generate_hp_after(nodes, T, alpha_after, theta_est, omega_est, mu_est, lam_stationary)\n",
    "    events_greedy = generate_hp_before(nodes, T, alpha_greedy, theta_est, omega_est, mu_est)\n",
    "    events_r = generate_hp_before(nodes, T, alpha_r, theta_r, omega_r, mu_r)\n",
    "    total_events_rot = simulate_rotation_policy(nodes, T, alpha_before, theta_est, omega_est, mu_est, 90)\n",
    "    \n",
    "    events_before = np.mean([len(events_before[i]) for i in range(nodes)])\n",
    "    events_after = np.mean([len(events_after[i]) for i in range(nodes)])\n",
    "    events_greedy = np.mean([len(events_greedy[i]) for i in range(nodes)])\n",
    "    events_r = np.mean([len(events_r[i]) for i in range(nodes)])\n",
    "    events_rotation = np.mean([len(i) for i in total_events_rot])\n",
    "    \n",
    "\n",
    "    \n",
    "    before_lenghts_array.append(events_before)\n",
    "    after_lenghts_array.append(events_after)\n",
    "    list_events_greedy.append(events_greedy)\n",
    "    list_events_random_perm.append(events_r)\n",
    "    rot.append(events_rotation)\n",
    "\n",
    "    \n",
    "    if sim % 100 == 0:\n",
    "        print('sim = ', sim)\n",
    "        print(f'before = {events_before}, after = {events_after}, greedy = {events_greedy}, random officer perm = {events_r}, rotation = {events_rotation}')\n",
    "\n",
    "print('standard = ' ,np.mean(before_lenghts_array))\n",
    "print('hawkes milp = ', np.mean(after_lenghts_array)) \n",
    "print('greedy = ', np.mean(list_events_greedy))\n",
    "print('random perm = ', np.mean(list_events_random_perm))  \n",
    "print('rotation = ', np.mean(rot))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f23fdc1-a888-48d0-a935-692b41a90051",
   "metadata": {},
   "source": [
    "### Dump and load the results in pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d415c12f-c554-46e4-b607-a43bb87de367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save standard\n",
    "#with open('standard.pkl', 'wb') as file:\n",
    "#    pickle.dump(before_lenghts_array, file)\n",
    "\n",
    "# Save omega_est\n",
    "#with open('milp', 'wb') as file:\n",
    "#    pickle.dump(after_lenghts_array, file)\n",
    "\n",
    "# Save greedy\n",
    "#with open('greedy.pkl', 'wb') as file:\n",
    "#    pickle.dump(list_events_greedy, file)\n",
    "\n",
    "# Save permutation\n",
    "#with open('perm.pkl', 'wb') as file:\n",
    "#    pickle.dump(list_events_random_perm, file)\n",
    "\n",
    "# Save rotation\n",
    "#with open('rot.pkl', 'wb') as file:\n",
    "#    pickle.dump(rot, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051fdfb6-876a-4db7-8d79-89e664a797fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('theta_est.pkl', 'rb') as file:\n",
    "#    theta_est = pickle.load(file)\n",
    "\n",
    "#with open('omega_est.pkl', 'rb') as file:\n",
    "#    omega_est = pickle.load(file)\n",
    "\n",
    "#with open('mu_est.pkl', 'rb') as file:\n",
    "#    mu_est = pickle.load(file)\n",
    "\n",
    "#with open('alpha_before.pkl', 'rb') as file:\n",
    "#   alpha_before = pickle.load(file)\n",
    "\n",
    "#with open('alpha_after.pkl', 'rb') as file:\n",
    "#    alpha_after = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c38238-3602-4ba5-8a4a-cbe4db33a22f",
   "metadata": {},
   "source": [
    "### Study assortativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb596ffa-22a3-4f13-983d-85abb3695c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta_correlation(alpha, theta):\n",
    "    \"\"\"\n",
    "    Compute Pearson correlation r between theta values\n",
    "    over all undirected edges in alpha.\n",
    "    \"\"\"\n",
    "    alpha = np.asarray(alpha)\n",
    "    theta = np.asarray(theta)\n",
    "\n",
    "    # Indices of upper-triangle edges (i<j)\n",
    "    i_idx, j_idx = np.where(np.triu(alpha, k=1) == 1)\n",
    "\n",
    "    # Corresponding theta values\n",
    "    ti = theta[i_idx]\n",
    "    tj = theta[j_idx]\n",
    "\n",
    "    # Pearson r\n",
    "    r, _ = pearsonr(ti, tj)\n",
    "    return r\n",
    "\n",
    "\n",
    "print(theta_correlation(alpha_before, theta_est), theta_correlation(alpha_after, theta_est))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc5228-5aec-4702-81dd-584e6ba66b69",
   "metadata": {},
   "source": [
    "### Plot the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706d1cab-eb67-4f78-931b-4e40a7740755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_network(alpha, target_node, filename = None):\n",
    "    np.random.seed(42)\n",
    "    # Ensure diagonal is 0\n",
    "    np.fill_diagonal(alpha, 0)\n",
    "    \n",
    "    # Create graph from adjacency matrix\n",
    "    G = nx.from_numpy_array(alpha)\n",
    "    \n",
    "    # Define node colors based on theta_estimated values\n",
    "    node_colors = ['blue' if theta_est[i] < 0.5 else 'red' for i in range(len(theta_est))]\n",
    "    \n",
    "    # Compute node positions (same layout for both graphs for consistency)\n",
    "    pos = nx.spring_layout(G)\n",
    "    \n",
    "    node_color=node_colors[target_node]\n",
    "    \n",
    "    # Get edges of the target node before optimization\n",
    "    edges = [(target_node, neighbor) for neighbor in G.neighbors(target_node)]\n",
    "    \n",
    "    # Plot before optimization\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    nx.draw(G, pos, node_color=node_colors, node_size=100, width=2, alpha=1)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=[target_node], node_color=node_color, edgecolors='black', node_size=300)  # Highlight node\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=edges, edge_color= 'orange', width=3)  # Highlight edges\n",
    "    #plt.savefig('chicago_after_optimization_node_highlight.eps')\n",
    "    if filename:\n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "        #print(f\"Figure saved as '{filename}'\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c7709-5fd6-4217-bc4f-1ddec55db4f4",
   "metadata": {},
   "source": [
    "### Network highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df1e256-4872-4dec-a1cf-9fb9339a813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_highlight(alpha, target_node, filename = None, title = False):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Ensure diagonal is 0\n",
    "    np.fill_diagonal(alpha, 0)\n",
    "    \n",
    "    # Create graph from adjacency matrix\n",
    "    G = nx.from_numpy_array(alpha)\n",
    "    \n",
    "    # Define node colors based on theta_estimated values\n",
    "    node_colors = ['blue' if theta_est[i] < 0.5 else 'red' for i in range(len(theta_est))]\n",
    "    \n",
    "    # Compute node positions (consistent layout)\n",
    "    pos = nx.spring_layout(G)\n",
    "\n",
    "    \n",
    "    # 1. Find the immediate neighbors of the target\n",
    "    neighbors = list(G.neighbors(target_node))\n",
    "    \n",
    "    # 2. Build the subgraph containing only the target + its neighbors\n",
    "    subgraph = G.subgraph([target_node] + neighbors)\n",
    "    \n",
    "    # 3. Prepare highlighting lists\n",
    "    highlight_nodes = [target_node] + neighbors\n",
    "    highlight_colors = [node_colors[target_node]] + [node_colors[n] for n in neighbors]\n",
    "    highlight_edges  = [(target_node, n) for n in neighbors]\n",
    "    \n",
    "    # 4. Plot just the subgraph\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # draw the non‐highlighted part (none here, since subgraph is just the star)\n",
    "    nx.draw_networkx_nodes(subgraph, pos, nodelist=highlight_nodes, node_color=highlight_colors, node_size=200)\n",
    "    nx.draw_networkx_edges(subgraph, pos, edgelist=highlight_edges, edge_color='orange', width=3)\n",
    "    # overlay the target node with a thick black circle\n",
    "    nx.draw_networkx_nodes(subgraph, pos, nodelist=[target_node], node_color=[node_colors[target_node]], edgecolors='black', \n",
    "                           linewidths=3, node_size=300)\n",
    "    if title:\n",
    "        plt.title(title, y = -0.1, fontsize = 20)\n",
    "    plt.axis('off')\n",
    "    if filename:\n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "        #print(f\"Figure saved as '{filename}'\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab7e881-3f53-4f93-86e0-8a4520270d0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_network(alpha_before, 12, 'chicago_graph_before.eps')\n",
    "plot_network(alpha_after, 12, 'chicago_graph_after.eps')\n",
    "plot_highlight(alpha_before, 12, 'chicago_highlight_graph_before.eps')\n",
    "plot_highlight(alpha_after, 12, 'chicago_highlight_graph_after.eps', 'Chicago')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
